{"ast":null,"code":"var _jsxFileName = \"/Users/craigpickard/Dev/web/hbo-client/src/components/FaceTracker.js\";\nimport React, { useEffect, useState } from \"react\";\nimport * as facemesh from \"@tensorflow-models/facemesh\";\nimport _isNil from \"lodash/isNil\";\nimport { TRIANGULATION } from \"./triangulation\";\nimport { findByLabelText } from \"@testing-library/react\"; // const useAnimationFrame = (callback) => {\n//   // Use useRef for mutable variables that we want to persist\n//   // without triggering a re-render on their change\n//   const requestRef = React.useRef();\n//   const previousTimeRef = React.useRef();\n//   const animate = (time) => {\n//     if (previousTimeRef.current != undefined) {\n//       const deltaTime = time - previousTimeRef.current;\n//       callback(deltaTime);\n//     }\n//     previousTimeRef.current = time;\n//     requestRef.current = requestAnimationFrame(animate);\n//   };\n//   React.useEffect(() => {\n//     requestRef.current = requestAnimationFrame(animate);\n//     return () => cancelAnimationFrame(requestRef.current);\n//   }, []); // Make sure the effect runs only once\n// };\n\nconst triangulateMesh = true;\n\nfunction drawPath(ctx, points, closePath) {\n  const region = new Path2D();\n  region.moveTo(points[0][0], points[0][1]);\n\n  for (let i = 1; i < points.length; i++) {\n    const point = points[i];\n    region.lineTo(point[0], point[1]);\n  }\n\n  if (closePath) {\n    region.closePath();\n  }\n\n  ctx.stroke(region);\n}\n\nconst FaceTracker = ({\n  videoRef,\n  userId,\n  stream\n}) => {\n  const [count, setCount] = React.useState(0);\n  const [trackingEnabled, setTrackingEnabled] = useState(true);\n  const [videoLoaded, setVideoLoaded] = useState(false);\n  const [uuid, setUuid] = useState(null);\n  let faces = [];\n  let model = null;\n  let ctx, videoWidth, videoHeight, video, canvas;\n  useEffect(() => {\n    console.log(\"useEffect FaceTracker\");\n    console.log({\n      stream\n    });\n    if (_isNil(stream)) return;\n\n    _init();\n  }, [stream]);\n\n  async function _init() {\n    model = await facemesh.load({\n      maxFaces: 1\n    }); // Pass in a video stream to the model to obtain\n    // an array of detected faces from the MediaPipe graph.\n    // video = document.querySelector(\"video\");\n\n    video = document.getElementById(`video-${userId}`);\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", async event => {\n      console.log(\"Yay! The readyState just increased to  \" + \"HAVE_CURRENT_DATA or greater for the first time.\");\n      videoWidth = video.videoWidth;\n      videoHeight = video.videoHeight;\n      video.width = videoWidth;\n      video.height = videoHeight;\n      canvas = document.getElementById(`output-${userId}`);\n      canvas.width = videoWidth;\n      canvas.height = videoHeight; // const canvasContainer = document.querySelector(\".canvas-wrapper\");\n      // canvasContainer.style = `width: ${videoWidth}px; height: ${videoHeight}px`;\n\n      ctx = canvas.getContext(\"2d\");\n      ctx.translate(canvas.width, 0);\n      ctx.scale(-1, 1);\n      ctx.fillStyle = \"#32EEDB\";\n      ctx.strokeStyle = \"#32EEDB\";\n      ctx.lineWidth = 0.5;\n      setVideoLoaded(true);\n      renderPrediction();\n    });\n  }\n\n  async function renderPrediction() {\n    // ctx.drawImage(\n    //   video,\n    //   0,\n    //   0,\n    //   videoWidth,\n    //   videoHeight,\n    //   0,\n    //   0,\n    //   canvas.width,\n    //   canvas.height\n    // );\n    if (trackingEnabled) {\n      const predictions = await model.estimateFaces(video);\n\n      if (predictions.length > 0) {\n        predictions.forEach(prediction => {\n          const keypoints = prediction.scaledMesh;\n\n          if (triangulateMesh) {\n            for (let i = 0; i < TRIANGULATION.length / 3; i++) {\n              const points = [TRIANGULATION[i * 3], TRIANGULATION[i * 3 + 1], TRIANGULATION[i * 3 + 2]].map(index => keypoints[index]);\n              drawPath(ctx, points, true);\n            }\n          } else {\n            for (let i = 0; i < keypoints.length; i++) {\n              const x = keypoints[i][0];\n              const y = keypoints[i][1];\n              ctx.beginPath();\n              ctx.arc(x, y, 1\n              /* radius */\n              , 0, 2 * Math.PI);\n              ctx.fill();\n            }\n          }\n        });\n      }\n    }\n\n    requestAnimationFrame(renderPrediction);\n  } //   useAnimationFrame(async (deltaTime) => {\n  //     // Pass on a function to the setter of the state\n  //     // to make sure we always have the latest state\n  //     console.log(\"animate\");\n  //     if (model !== null) faces = await model.estimateFaces(video);\n  //     // faces.forEach((face) => console.log(face.scaledMesh));\n  //     console.log(faces.length);\n  //   });\n\n\n  return /*#__PURE__*/React.createElement(\"div\", {\n    style: {\n      display: \"flex\",\n      flex: 1,\n      border: \"2px solid red\",\n      position: \"relative\",\n      alignItems: \"center\",\n      justifyContent: \"center\"\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 152,\n      columnNumber: 5\n    }\n  }, /*#__PURE__*/React.createElement(\"video\", {\n    id: `video-${userId}`,\n    autoPlay: true,\n    muted: true // ref={videoRef}\n    ,\n    playsInline: true,\n    style: {\n      WebkitTransform: \"scaleX(-1)\",\n      transform: \"scaleX(-1)\",\n      visibility: \"hidden\",\n      // display: \"none\",\n      width: \"auto\",\n      height: \"auto\",\n      border: \"3px solid green\"\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 162,\n      columnNumber: 7\n    }\n  }), /*#__PURE__*/React.createElement(\"canvas\", {\n    id: `output-${userId}` // style={{ position: \"absolute\", top: 0, left: 0, zIndex: 1000 }}\n    ,\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 178,\n      columnNumber: 7\n    }\n  }));\n};\n\nexport default FaceTracker;","map":{"version":3,"sources":["/Users/craigpickard/Dev/web/hbo-client/src/components/FaceTracker.js"],"names":["React","useEffect","useState","facemesh","_isNil","TRIANGULATION","findByLabelText","triangulateMesh","drawPath","ctx","points","closePath","region","Path2D","moveTo","i","length","point","lineTo","stroke","FaceTracker","videoRef","userId","stream","count","setCount","trackingEnabled","setTrackingEnabled","videoLoaded","setVideoLoaded","uuid","setUuid","faces","model","videoWidth","videoHeight","video","canvas","console","log","_init","load","maxFaces","document","getElementById","srcObject","addEventListener","event","width","height","getContext","translate","scale","fillStyle","strokeStyle","lineWidth","renderPrediction","predictions","estimateFaces","forEach","prediction","keypoints","scaledMesh","map","index","x","y","beginPath","arc","Math","PI","fill","requestAnimationFrame","display","flex","border","position","alignItems","justifyContent","WebkitTransform","transform","visibility"],"mappings":";AAAA,OAAOA,KAAP,IAAgBC,SAAhB,EAA2BC,QAA3B,QAA2C,OAA3C;AACA,OAAO,KAAKC,QAAZ,MAA0B,6BAA1B;AACA,OAAOC,MAAP,MAAmB,cAAnB;AAEA,SAASC,aAAT,QAA8B,iBAA9B;AACA,SAASC,eAAT,QAAgC,wBAAhC,C,CAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;;AAEA,MAAMC,eAAe,GAAG,IAAxB;;AAEA,SAASC,QAAT,CAAkBC,GAAlB,EAAuBC,MAAvB,EAA+BC,SAA/B,EAA0C;AACxC,QAAMC,MAAM,GAAG,IAAIC,MAAJ,EAAf;AACAD,EAAAA,MAAM,CAACE,MAAP,CAAcJ,MAAM,CAAC,CAAD,CAAN,CAAU,CAAV,CAAd,EAA4BA,MAAM,CAAC,CAAD,CAAN,CAAU,CAAV,CAA5B;;AACA,OAAK,IAAIK,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGL,MAAM,CAACM,MAA3B,EAAmCD,CAAC,EAApC,EAAwC;AACtC,UAAME,KAAK,GAAGP,MAAM,CAACK,CAAD,CAApB;AACAH,IAAAA,MAAM,CAACM,MAAP,CAAcD,KAAK,CAAC,CAAD,CAAnB,EAAwBA,KAAK,CAAC,CAAD,CAA7B;AACD;;AAED,MAAIN,SAAJ,EAAe;AACbC,IAAAA,MAAM,CAACD,SAAP;AACD;;AACDF,EAAAA,GAAG,CAACU,MAAJ,CAAWP,MAAX;AACD;;AAED,MAAMQ,WAAW,GAAG,CAAC;AAAEC,EAAAA,QAAF;AAAYC,EAAAA,MAAZ;AAAoBC,EAAAA;AAApB,CAAD,KAAkC;AACpD,QAAM,CAACC,KAAD,EAAQC,QAAR,IAAoBzB,KAAK,CAACE,QAAN,CAAe,CAAf,CAA1B;AACA,QAAM,CAACwB,eAAD,EAAkBC,kBAAlB,IAAwCzB,QAAQ,CAAC,IAAD,CAAtD;AACA,QAAM,CAAC0B,WAAD,EAAcC,cAAd,IAAgC3B,QAAQ,CAAC,KAAD,CAA9C;AACA,QAAM,CAAC4B,IAAD,EAAOC,OAAP,IAAkB7B,QAAQ,CAAC,IAAD,CAAhC;AAEA,MAAI8B,KAAK,GAAG,EAAZ;AACA,MAAIC,KAAK,GAAG,IAAZ;AACA,MAAIxB,GAAJ,EAASyB,UAAT,EAAqBC,WAArB,EAAkCC,KAAlC,EAAyCC,MAAzC;AAEApC,EAAAA,SAAS,CAAC,MAAM;AACdqC,IAAAA,OAAO,CAACC,GAAR,CAAY,uBAAZ;AACAD,IAAAA,OAAO,CAACC,GAAR,CAAY;AAAEhB,MAAAA;AAAF,KAAZ;AACA,QAAInB,MAAM,CAACmB,MAAD,CAAV,EAAoB;;AACpBiB,IAAAA,KAAK;AACN,GALQ,EAKN,CAACjB,MAAD,CALM,CAAT;;AAOA,iBAAeiB,KAAf,GAAuB;AACrBP,IAAAA,KAAK,GAAG,MAAM9B,QAAQ,CAACsC,IAAT,CAAc;AAAEC,MAAAA,QAAQ,EAAE;AAAZ,KAAd,CAAd,CADqB,CAGrB;AACA;AACA;;AACAN,IAAAA,KAAK,GAAGO,QAAQ,CAACC,cAAT,CAAyB,SAAQtB,MAAO,EAAxC,CAAR;AACAc,IAAAA,KAAK,CAACS,SAAN,GAAkBtB,MAAlB;AACAa,IAAAA,KAAK,CAACU,gBAAN,CAAuB,YAAvB,EAAqC,MAAOC,KAAP,IAAiB;AACpDT,MAAAA,OAAO,CAACC,GAAR,CACE,4CACE,kDAFJ;AAIAL,MAAAA,UAAU,GAAGE,KAAK,CAACF,UAAnB;AACAC,MAAAA,WAAW,GAAGC,KAAK,CAACD,WAApB;AACAC,MAAAA,KAAK,CAACY,KAAN,GAAcd,UAAd;AACAE,MAAAA,KAAK,CAACa,MAAN,GAAed,WAAf;AAEAE,MAAAA,MAAM,GAAGM,QAAQ,CAACC,cAAT,CAAyB,UAAStB,MAAO,EAAzC,CAAT;AACAe,MAAAA,MAAM,CAACW,KAAP,GAAed,UAAf;AACAG,MAAAA,MAAM,CAACY,MAAP,GAAgBd,WAAhB,CAZoD,CAapD;AACA;;AAEA1B,MAAAA,GAAG,GAAG4B,MAAM,CAACa,UAAP,CAAkB,IAAlB,CAAN;AACAzC,MAAAA,GAAG,CAAC0C,SAAJ,CAAcd,MAAM,CAACW,KAArB,EAA4B,CAA5B;AACAvC,MAAAA,GAAG,CAAC2C,KAAJ,CAAU,CAAC,CAAX,EAAc,CAAd;AACA3C,MAAAA,GAAG,CAAC4C,SAAJ,GAAgB,SAAhB;AACA5C,MAAAA,GAAG,CAAC6C,WAAJ,GAAkB,SAAlB;AACA7C,MAAAA,GAAG,CAAC8C,SAAJ,GAAgB,GAAhB;AACA1B,MAAAA,cAAc,CAAC,IAAD,CAAd;AACA2B,MAAAA,gBAAgB;AACjB,KAxBD;AAyBD;;AAED,iBAAeA,gBAAf,GAAkC;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA,QAAI9B,eAAJ,EAAqB;AACnB,YAAM+B,WAAW,GAAG,MAAMxB,KAAK,CAACyB,aAAN,CAAoBtB,KAApB,CAA1B;;AACA,UAAIqB,WAAW,CAACzC,MAAZ,GAAqB,CAAzB,EAA4B;AAC1ByC,QAAAA,WAAW,CAACE,OAAZ,CAAqBC,UAAD,IAAgB;AAClC,gBAAMC,SAAS,GAAGD,UAAU,CAACE,UAA7B;;AACA,cAAIvD,eAAJ,EAAqB;AACnB,iBAAK,IAAIQ,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGV,aAAa,CAACW,MAAd,GAAuB,CAA3C,EAA8CD,CAAC,EAA/C,EAAmD;AACjD,oBAAML,MAAM,GAAG,CACbL,aAAa,CAACU,CAAC,GAAG,CAAL,CADA,EAEbV,aAAa,CAACU,CAAC,GAAG,CAAJ,GAAQ,CAAT,CAFA,EAGbV,aAAa,CAACU,CAAC,GAAG,CAAJ,GAAQ,CAAT,CAHA,EAIbgD,GAJa,CAIRC,KAAD,IAAWH,SAAS,CAACG,KAAD,CAJX,CAAf;AAMAxD,cAAAA,QAAQ,CAACC,GAAD,EAAMC,MAAN,EAAc,IAAd,CAAR;AACD;AACF,WAVD,MAUO;AACL,iBAAK,IAAIK,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG8C,SAAS,CAAC7C,MAA9B,EAAsCD,CAAC,EAAvC,EAA2C;AACzC,oBAAMkD,CAAC,GAAGJ,SAAS,CAAC9C,CAAD,CAAT,CAAa,CAAb,CAAV;AACA,oBAAMmD,CAAC,GAAGL,SAAS,CAAC9C,CAAD,CAAT,CAAa,CAAb,CAAV;AAEAN,cAAAA,GAAG,CAAC0D,SAAJ;AACA1D,cAAAA,GAAG,CAAC2D,GAAJ,CAAQH,CAAR,EAAWC,CAAX,EAAc;AAAE;AAAhB,gBAA8B,CAA9B,EAAiC,IAAIG,IAAI,CAACC,EAA1C;AACA7D,cAAAA,GAAG,CAAC8D,IAAJ;AACD;AACF;AACF,SAtBD;AAuBD;AACF;;AAEDC,IAAAA,qBAAqB,CAAChB,gBAAD,CAArB;AACD,GA/FmD,CAiGpD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AAEA,sBACE;AACE,IAAA,KAAK,EAAE;AACLiB,MAAAA,OAAO,EAAE,MADJ;AAELC,MAAAA,IAAI,EAAE,CAFD;AAGLC,MAAAA,MAAM,EAAE,eAHH;AAILC,MAAAA,QAAQ,EAAE,UAJL;AAKLC,MAAAA,UAAU,EAAE,QALP;AAMLC,MAAAA,cAAc,EAAE;AANX,KADT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAUE;AACE,IAAA,EAAE,EAAG,SAAQxD,MAAO,EADtB;AAEE,IAAA,QAAQ,MAFV;AAGE,IAAA,KAAK,MAHP,CAIE;AAJF;AAKE,IAAA,WAAW,MALb;AAME,IAAA,KAAK,EAAE;AACLyD,MAAAA,eAAe,EAAE,YADZ;AAELC,MAAAA,SAAS,EAAE,YAFN;AAGLC,MAAAA,UAAU,EAAE,QAHP;AAIL;AACAjC,MAAAA,KAAK,EAAE,MALF;AAMLC,MAAAA,MAAM,EAAE,MANH;AAOL0B,MAAAA,MAAM,EAAE;AAPH,KANT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAVF,eA0BE;AACE,IAAA,EAAE,EAAG,UAASrD,MAAO,EADvB,CAEE;AAFF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IA1BF,CADF;AAiCD,CA3ID;;AA6IA,eAAeF,WAAf","sourcesContent":["import React, { useEffect, useState } from \"react\";\nimport * as facemesh from \"@tensorflow-models/facemesh\";\nimport _isNil from \"lodash/isNil\";\n\nimport { TRIANGULATION } from \"./triangulation\";\nimport { findByLabelText } from \"@testing-library/react\";\n\n// const useAnimationFrame = (callback) => {\n//   // Use useRef for mutable variables that we want to persist\n//   // without triggering a re-render on their change\n//   const requestRef = React.useRef();\n//   const previousTimeRef = React.useRef();\n\n//   const animate = (time) => {\n//     if (previousTimeRef.current != undefined) {\n//       const deltaTime = time - previousTimeRef.current;\n//       callback(deltaTime);\n//     }\n//     previousTimeRef.current = time;\n//     requestRef.current = requestAnimationFrame(animate);\n//   };\n\n//   React.useEffect(() => {\n//     requestRef.current = requestAnimationFrame(animate);\n//     return () => cancelAnimationFrame(requestRef.current);\n//   }, []); // Make sure the effect runs only once\n// };\n\nconst triangulateMesh = true;\n\nfunction drawPath(ctx, points, closePath) {\n  const region = new Path2D();\n  region.moveTo(points[0][0], points[0][1]);\n  for (let i = 1; i < points.length; i++) {\n    const point = points[i];\n    region.lineTo(point[0], point[1]);\n  }\n\n  if (closePath) {\n    region.closePath();\n  }\n  ctx.stroke(region);\n}\n\nconst FaceTracker = ({ videoRef, userId, stream }) => {\n  const [count, setCount] = React.useState(0);\n  const [trackingEnabled, setTrackingEnabled] = useState(true);\n  const [videoLoaded, setVideoLoaded] = useState(false);\n  const [uuid, setUuid] = useState(null);\n\n  let faces = [];\n  let model = null;\n  let ctx, videoWidth, videoHeight, video, canvas;\n\n  useEffect(() => {\n    console.log(\"useEffect FaceTracker\");\n    console.log({ stream });\n    if (_isNil(stream)) return;\n    _init();\n  }, [stream]);\n\n  async function _init() {\n    model = await facemesh.load({ maxFaces: 1 });\n\n    // Pass in a video stream to the model to obtain\n    // an array of detected faces from the MediaPipe graph.\n    // video = document.querySelector(\"video\");\n    video = document.getElementById(`video-${userId}`);\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", async (event) => {\n      console.log(\n        \"Yay! The readyState just increased to  \" +\n          \"HAVE_CURRENT_DATA or greater for the first time.\"\n      );\n      videoWidth = video.videoWidth;\n      videoHeight = video.videoHeight;\n      video.width = videoWidth;\n      video.height = videoHeight;\n\n      canvas = document.getElementById(`output-${userId}`);\n      canvas.width = videoWidth;\n      canvas.height = videoHeight;\n      // const canvasContainer = document.querySelector(\".canvas-wrapper\");\n      // canvasContainer.style = `width: ${videoWidth}px; height: ${videoHeight}px`;\n\n      ctx = canvas.getContext(\"2d\");\n      ctx.translate(canvas.width, 0);\n      ctx.scale(-1, 1);\n      ctx.fillStyle = \"#32EEDB\";\n      ctx.strokeStyle = \"#32EEDB\";\n      ctx.lineWidth = 0.5;\n      setVideoLoaded(true);\n      renderPrediction();\n    });\n  }\n\n  async function renderPrediction() {\n    // ctx.drawImage(\n    //   video,\n    //   0,\n    //   0,\n    //   videoWidth,\n    //   videoHeight,\n    //   0,\n    //   0,\n    //   canvas.width,\n    //   canvas.height\n    // );\n\n    if (trackingEnabled) {\n      const predictions = await model.estimateFaces(video);\n      if (predictions.length > 0) {\n        predictions.forEach((prediction) => {\n          const keypoints = prediction.scaledMesh;\n          if (triangulateMesh) {\n            for (let i = 0; i < TRIANGULATION.length / 3; i++) {\n              const points = [\n                TRIANGULATION[i * 3],\n                TRIANGULATION[i * 3 + 1],\n                TRIANGULATION[i * 3 + 2],\n              ].map((index) => keypoints[index]);\n\n              drawPath(ctx, points, true);\n            }\n          } else {\n            for (let i = 0; i < keypoints.length; i++) {\n              const x = keypoints[i][0];\n              const y = keypoints[i][1];\n\n              ctx.beginPath();\n              ctx.arc(x, y, 1 /* radius */, 0, 2 * Math.PI);\n              ctx.fill();\n            }\n          }\n        });\n      }\n    }\n\n    requestAnimationFrame(renderPrediction);\n  }\n\n  //   useAnimationFrame(async (deltaTime) => {\n  //     // Pass on a function to the setter of the state\n  //     // to make sure we always have the latest state\n  //     console.log(\"animate\");\n  //     if (model !== null) faces = await model.estimateFaces(video);\n  //     // faces.forEach((face) => console.log(face.scaledMesh));\n  //     console.log(faces.length);\n  //   });\n\n  return (\n    <div\n      style={{\n        display: \"flex\",\n        flex: 1,\n        border: \"2px solid red\",\n        position: \"relative\",\n        alignItems: \"center\",\n        justifyContent: \"center\",\n      }}\n    >\n      <video\n        id={`video-${userId}`}\n        autoPlay\n        muted\n        // ref={videoRef}\n        playsInline\n        style={{\n          WebkitTransform: \"scaleX(-1)\",\n          transform: \"scaleX(-1)\",\n          visibility: \"hidden\",\n          // display: \"none\",\n          width: \"auto\",\n          height: \"auto\",\n          border: \"3px solid green\",\n        }}\n      />\n      <canvas\n        id={`output-${userId}`}\n        // style={{ position: \"absolute\", top: 0, left: 0, zIndex: 1000 }}\n      />\n    </div>\n  );\n};\n\nexport default FaceTracker;\n"]},"metadata":{},"sourceType":"module"}